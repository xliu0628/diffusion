{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a diffusion model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The notebook is copied from https://huggingface.co/docs/diffusers/en/tutorials/basic_training).\n",
    "\n",
    "Unconditional image generation is a popular application of diffusion models that generates images that look like those in dataset used for training. Typlically, the best results are obtained from finetuning a pretrained model on a specific dataset. You can find many of these checkpoints on the [Hub](https://huggingface.co/search/full-text?q=unconditional-image-generation&type=model), but if you can't find one you like, you can always train your own! \n",
    "\n",
    "This tutorial will teach you how to train a [UNet2DModel](https://huggingface.co/docs/diffusers/v0.32.2/en/api/models/unet2d#diffusers.UNet2DModel) from scratch on a subset of the [Smithsonian Butterflies] (https://huggingface.co/datasets/huggan/smithsonian_butterflies_subset) dataset to generate your own butterflies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This training tutorial is based on the [Training with Diffusers](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/training_example.ipynb) notebook. For additional details and context about diffusion models like how they work, check out the notebook. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you begin, make sure you have `Datasets` installed to load and preprocess image datasets, and `Accelerate` to simplify training on any number of GPUs. The following command will also install [TensorBoard](https://www.tensorflow.org/tensorboard) to visualize training metrics (you can also use [Weights and Basics] (https://docs.wandb.ai/) to track your training). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diffusers[training]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training configuration  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience create a `TrainingConfig` class containing the training hyperparameters (feel free to adjust them):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclasses\n",
    "\n",
    "@dataclasses\n",
    "class TrainingConfig: \n",
    "    image_size = 128 # the genrated image resolution\n",
    "    train_batch_size = 16\n",
    "    eval_batch_size = 16 # how many images to sample during evaluation\n",
    "    num_epochs = 50\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    save_image_epochs = 10\n",
    "    save_model_epochs = 30\n",
    "    mixed_precision = 'fp16'   # 'no' for float32, 'fp16' for automatic mixed precision\n",
    "    output_dir = 'ddpm-butterflies-128'\n",
    "\n",
    "    push_to_hub = False   \n",
    "    hub_model_id = '<your-username>'/<my-awesome-model>\n",
    "    hub_private_repo = None\n",
    "    overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
    "    seed = 0\n",
    "\n",
    "config = TrainingConfig()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily load the Smithsonian Butterflies dataset with the ðŸ¤— Datasets library:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_dataset\n",
    "config.dataset_name = \"huggan/smithsonian_butterflies_subset\"\n",
    "dataset = load_dataset(config.dataset_name,split='train')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ¤— Datasets uses the [Image](https://huggingface.co/docs/datasets/v3.2.0/en/package_reference/main_classes#datasets.Image) feature to automatically decode the image data and load it as a [PIL.Image](https://pillow.readthedocs.io/en/stable/reference/Image.html) which we can visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for i, image in enumerate(dataset[:4][\"image\"]):\n",
    "    axs[i].imshow(image)\n",
    "    axs[i].set_axis_off()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data\n",
    "\n",
    "The images are all different sizes though, so youâ€™ll need to preprocess them first:\n",
    "- `Resize` changes the image size to the one defined in `config.image_size`\n",
    "- `RandomHorizontalFlip` augments the dataset by randomly mirroring the images\n",
    "- `Normalize` is important to rescale the pixel values into a [-1,1] range, which is what the model expects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((config.image_size, config.image_size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5],[0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Datasets' [set_transform](https://huggingface.co/docs/datasets/v3.2.0/en/package_reference/main_classes#datasets.Dataset.set_transform) method to apply the `preprocess` function on the fly during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(examples):\n",
    "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return {\"images\": images}\n",
    "\n",
    "dataset.set_transform(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now youâ€™re ready to wrap the dataset in a [DataLoader](https://pytorch.org/docs/stable/data#torch.utils.data.DataLoader) for training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a UNet2D Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained models in Diffusers are easily created from their model class with the parameters you want. For example, to create a `UNet2DModel`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel\n",
    "\n",
    "model = UNet2DModel(\n",
    "    sample_size = config.sample_size,\n",
    "    in_channels = 3, \n",
    "    out_channels = 3,\n",
    "    layers_per_block = 2, # how many ResNet layers to use per U-Net block\n",
    "    block_out_channels = (128, 128, 256, 256, 512, 512), # number of output channels in each U-Net block\n",
    "    down_block_types = (\n",
    "        'Downblock2D', ## a regular Resnet downsampling block.\n",
    "        'Downblock2D',\n",
    "        'Downblock2D',\n",
    "        'Downblock2D',\n",
    "        'AttnDownblock2D',\n",
    "        'Downblock2D',\n",
    "    )\n",
    "    up_block_types = (\n",
    "        'Upblock2D',\n",
    "        'AttnUpblock2D',\n",
    "        'Upblock2D',\n",
    "        'Upblock2D',\n",
    "        'Upblock2D',\n",
    "        'Upblock2D',\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often a good idea to quickly check the sample image shape matches the model output shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = dataset[0]['images'].unsqueeze(0)\n",
    "print(\"Input shape:\", sample_image.shape)\n",
    "\n",
    "print(\"Output shape:\"), model(sample_image, timestep=0).sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a scheduler\n",
    "You'll need a scheduler to add some noise to the images. \n",
    "The scheduler behaves differently depending on whether you're using the model for training or inference. During inference, the scheduler generates the image from the noise. During training, the scheduler takes a model output -- or a sample -- from a specific point in the diffusion process and applies noise to the image according to a noise scheduler and an update rule. \n",
    "\n",
    "Let's take a look at the [DDPMScheduler](https://huggingface.co/docs/diffusers/v0.32.2/en/api/schedulers/ddpm#diffusers.DDPMScheduler) and use the `add_noise` method to add some random noise to the sample image from before: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from diffusers import DDPMScheduler\n",
    "\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "noise = torch.randn(sample_image.shape)\n",
    "timesteps = torch.LongTensor([50])\n",
    "noisy_image = noise_scheduler.add_noise(sample_iamge,noise,timesteps)\n",
    "\n",
    "Image.fromarray(((noisy_image.permute(0, 2, 3, 1) + 1.0) * 127.5).type(torch.uint8).numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training objective of the model is to predict the noise added to the image. The loss at this step can be calculated as: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "noise_pred = model(noisy_image, timesteps).sample\n",
    "loss = F.mse_loss(noise_pred, noise)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=config.learning_rate)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer, \n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(len(train_dataloader) * config.num_epochs),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you'll need a way to evaluate the model. For evaluation, you can use the [DDPMPipeline](https://huggingface.co/docs/diffusers/v0.32.2/en/api/pipelines/ddpm#diffusers.DDPMPipeline) to generate a batch of sample images and save it as a grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMPipeline\n",
    "from diffusers.util import make_image_grid\n",
    "import os\n",
    "\n",
    "def evaluate(config, epoch, pipeline):\n",
    "    ## sample some images from random noise (this is the backward diffusion process).\n",
    "    # The default pipeline output type is 'List[PIL.Image]'\n",
    "    images = pipeline(\n",
    "        batch_size=config.eval_batch_size,\n",
    "        generator=torch.Generator(device='cpu').manual_seed(config.seed), ## use a separate torch generator to avoid rewinding the random state of the main training loop\n",
    "    ).images\n",
    "\n",
    "    # make a grid out of the images\n",
    "    image_grid = make_image_grid(images, rows=4, cols=4)\n",
    "\n",
    "    # save the images\n",
    "    test_dir = os.path.join(config.output_dir, \"test_images\")\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can wrp all these components together in a training loop with `Accelerate` for easy `TensorBoard` logging, gradient accumulation, and mixed precision training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n",
    "    # initialize accelerator and tensorboard logging\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        log_with='tensorboard',\n",
    "        project_dir=os.path.join(config.output_dir, \"logs\"),\n",
    "    )\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        if config.output_dir is not None:\n",
    "            os.makedirs(config.output_dir, exist_ok=True)\n",
    "            if config.push_to_hub:\n",
    "                repo_id = create_repo(\n",
    "                    repo_id=config.hub_model_id or Path(config.output_dir).name, exist_ok=True\n",
    "                    ).repo_id\n",
    "            accelerator.init_trackers('train_example')\n",
    "    \n",
    "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    # Now you train the model\n",
    "    for epoch in range(config.num_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            clean_images = batch[\"images\"]\n",
    "            # sample noise to add to the image\n",
    "            noise = torch.randn(clean_images.shape, device=clean_images.device)\n",
    "            bs = clean_images.shape[0]\n",
    "        \n",
    "            # sample a random timestep for each image\n",
    "            timesteps = torch.randint(0, noise_scheduler.num_timesteps, (bs,), device=clean_images.device,\n",
    "                                  dtype=torch.int64\n",
    "            )\n",
    "\n",
    "        # Add noise to the clean images according to the noise magnitude at each timestep \n",
    "        # this is the forward diffusion process\n",
    "        noise_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "        with accelerator.accumulate(model):\n",
    "            # predict the noise schedule\n",
    "            noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            if accelerator.sync_gradients:\n",
    "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        progress_bar.update(1)\n",
    "        logs = {'loss': loss.detach().item(), 'lr':lr_scheduler.get_last_lr()[0], 'step': global_step}\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        accelerator.log(logs, step=global_step)\n",
    "        global_step += 1\n",
    "\n",
    "    # After each epoch you optionally sample some demo images with evaluate() and save the model\n",
    "    if accelerator.is_main_process:\n",
    "        pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n",
    "\n",
    "        if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "            if config.push_to_hub:\n",
    "                upload_folder(\n",
    "                    repo_id=repo_id,\n",
    "                    folder_path=config.output_dir,\n",
    "                    commit_message=f\"Epoch {epoch}\",\n",
    "                    ignore_patterns=[\"step_*\",\"epoch_*\"],\n",
    "                )\n",
    "            else:\n",
    "                pipeline.save_pretrained(config.output_dir)\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "args = (config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\n",
    "notebook_launcher(train_loop, args, num_processes=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training is complete, take a look at the final iamges generated by your diffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "sampel_images = sorted(glob.glob(f\"{config.output_dir}/samples/*.png\"))\n",
    "Image.open(sample_images[-1])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
